

@misc{ahn-21,
  author =        {Byron Ahn and Nanette Veilleux and
                   Stefanie {Shattuck-Hufnagel} and Alejna Brugos},
  howpublished =  {Available at \url{https://osf.io/usbx5/}},
  title =         {{PoLaR} Annotation Guidelines (version 1.0)},
  year =          {2021},
  doi =           {10.17605/OSF.IO/USBX5},
}

@misc{mcauliffe-19,
  author =        {McAuliffe, Michael and Michaela Socolof and
                   Sarah Mihuc and Michael Wagner and
                   Morgan Sonderegger},
  howpublished =  {Retrieved from
  {\url{http://montrealcorpustools.github.io/Montreal-Forced-Aligner/}}},
  month =         {April},
  title =         {Montreal Forced Aligner [Computer Program] (version
                   1.0.1)},
  year =          {2019},
  doi =           {http://doi.org/10.5281/zenodo.2630943},
}

@article{cole-14,
  author =        {Jennifer Cole and Timothy Mahrt and Jos√© I. Hualde},
  journal =       {Proceedings of Speech Prosody 7},
  pages =         {859-863},
  title =         {Listening for sound, listening for meaning: Task
                   effects on prosodic transcription},
  year =          {2014},
}

@article{cole-17,
  author =        {Jennifer Cole and Timothy Mahrt and Joseph Roy},
  journal =       {Computer Speech and Language},
  pages =         {300--325},
  title =         {Crowd-sourcing prosodic annotation},
  volume =        {45},
  year =          {2017},
  abstract =      {Much of what is known about prosody is based on
                   native speaker intuitions of idealized speech, or on
                   prosodic annotations from trained annotators whose
                   auditory impressions are augmented by visual evidence
                   from speech waveforms, spectrograms and pitch tracks.
                   Expanding the prosodic data currently available to
                   cover more languages, and to cover a broader range of
                   unscripted speech styles, is prohibitive due to the
                   time, money and human expertise needed for prosodic
                   annotation. We describe an alternative approach to
                   prosodic data collection, with coarse-grained
                   annotations from a cohort of untrained annotators
                   performing rapid prosody transcription (RPT) using
                   LMEDS, an open-source software tool we developed to
                   enable large-scale, crowd-sourced data collection
                   with RPT. Results from three RPT experiments are
                   reported. The reliability of RPT is analysed
                   comparing kappa statistics for lab-based and
                   crowd-sourced annotations for American English,
                   comparing annotators from the same (US) versus
                   different (Indian) dialect groups, and comparing each
                   RPT annotator with a ToBI annotation. Results show
                   better reliability for same-dialect annotators (US),
                   and the best overall reliability from crowd-sourced
                   US annotators, though lab-based annotations are the
                   most similar to ToBI annotations. A generalized
                   additive mixed model is used to test differences
                   among annotator groups in the factors that predict
                   prosodic annotation. Results show that a common set
                   of acoustic and contextual factors predict prosodic
                   labels for all annotator groups, with only small
                   differences among the RPT groups, but with larger
                   effects on prosodic marking for ToBI annotators. The
                   findings suggest methods for optimizing the
                   efficiency of RPT annotations. Overall, crowd-sourced
                   prosodic annotation is shown to be efficient, and to
                   rely on established cues to prosody, supporting its
                   use for prosody research across languages, dialects,
                   speaker populations, and speech genres.},
  doi =           {10.1016/j.csl.2017.02.008},
}

